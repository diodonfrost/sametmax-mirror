<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" 
	>
<channel>
	<title>Comments on: Parser du HTML avec BeautifulSoup</title>
	<atom:link href="http://sametmax.com/parser-du-html-avec-beautifulsoup/feed/" rel="self" type="application/rss+xml" />
	<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/</link>
	<description>Du code, du cul</description>
	<lastBuildDate>Fri, 06 Sep 2019 09:34:15 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.7</generator>
	<item>
		<title>By: pascal</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-187181</link>
		<dc:creator><![CDATA[pascal]]></dc:creator>
		<pubDate>Fri, 12 May 2017 09:25:37 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-187181</guid>
		<description><![CDATA[Bonjour

Débutant en python 3 sous anaconda, j&#039;ai fais un programme qui utilise BS pour trouver les liens contact dans une page web valide.

Selon les URL testées, le lien contact apparait bien dans la console Ipython d&#039;Anaconda mais je ne sais pas comment retrouver la chaine avec python dans la liste de façon à ce que la fonction me retourne le lien.

A priori ce qui est dans la liste n&#039;est pas une chaine mais je ne sais pas comment transformer chaque élément de la liste en chaine pour le tester,

avez vous une idée .

merci d&#039;avance

Le code de la fonction appelée est le suivant :

import urllib.request, urllib.error, urllib.parse

import bs4

import os

import csv

définition des fonctions

def TestAccessible(the_uri):

l&#039;URL du site web est valide, on va chercher s&#039;il existe un lien vers la

page contact et retourner ce lien

&lt;pre&gt;&lt;code&gt;html_page = urllib.request.urlopen(the_uri)
#response is now a string you can search through containing the page&#039;s html
soup = bs4.BeautifulSoup(html_page, &#039;html.parser&#039;)
LinkList = []
for Link in soup.find_all(&#039;a&#039;):
    LinkFound=Link.get(&#039;href&#039;)
    LinkList.append(LinkFound)
print (&quot;nombre d&#039;URL : &quot;,len(LinkList))
print (LinkList)
&lt;/code&gt;&lt;/pre&gt;

&quot;&quot;&quot;    chaine = &quot;-&quot;.join(LinkList)

&lt;pre&gt;&lt;code&gt;if (&quot;contact&quot; in chaine) :
    print (&quot;URL Contact&quot;)
&lt;/code&gt;&lt;/pre&gt;

&quot;&quot;&quot;
]]></description>
		<content:encoded><![CDATA[<p>Bonjour</p>
<p>Débutant en python 3 sous anaconda, j&#8217;ai fais un programme qui utilise BS pour trouver les liens contact dans une page web valide.</p>
<p>Selon les URL testées, le lien contact apparait bien dans la console Ipython d&#8217;Anaconda mais je ne sais pas comment retrouver la chaine avec python dans la liste de façon à ce que la fonction me retourne le lien.</p>
<p>A priori ce qui est dans la liste n&#8217;est pas une chaine mais je ne sais pas comment transformer chaque élément de la liste en chaine pour le tester,</p>
<p>avez vous une idée .</p>
<p>merci d&#8217;avance</p>
<p>Le code de la fonction appelée est le suivant :</p>
<p>import urllib.request, urllib.error, urllib.parse</p>
<p>import bs4</p>
<p>import os</p>
<p>import csv</p>
<p>définition des fonctions</p>
<p>def TestAccessible(the_uri):</p>
<p>l&#8217;URL du site web est valide, on va chercher s&#8217;il existe un lien vers la</p>
<p>page contact et retourner ce lien</p>
<pre><code>html_page = urllib.request.urlopen(the_uri)
#response is now a string you can search through containing the page's html
soup = bs4.BeautifulSoup(html_page, 'html.parser')
LinkList = []
for Link in soup.find_all('a'):
    LinkFound=Link.get('href')
    LinkList.append(LinkFound)
print ("nombre d'URL : ",len(LinkList))
print (LinkList)
</code></pre>
<p>&#8220;&#8221;&#8221;    chaine = &#8220;-&#8220;.join(LinkList)</p>
<pre><code>if ("contact" in chaine) :
    print ("URL Contact")
</code></pre>
<p>&#8220;&#8221;&#8221;</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Sam</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-162397</link>
		<dc:creator><![CDATA[Sam]]></dc:creator>
		<pubDate>Sun, 21 Jun 2015 19:48:40 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-162397</guid>
		<description><![CDATA[&lt;p&gt;Parce que lxml ne gère pas les HTML mal formé aussi bien que beautiful soup qui peut avaler des trucs vraiment dégueulasses.&lt;/p&gt;
]]></description>
		<content:encoded><![CDATA[<p>Parce que lxml ne gère pas les HTML mal formé aussi bien que beautiful soup qui peut avaler des trucs vraiment dégueulasses.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: bugs</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-162395</link>
		<dc:creator><![CDATA[bugs]]></dc:creator>
		<pubDate>Sun, 21 Jun 2015 19:36:26 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-162395</guid>
		<description><![CDATA[Pourquoi ne pas utiliser lxml?

Ça permettrais de faire des recherches avec xpath, qui est quand même plus simple/souple surtout pour des sites mal foutu (par exemple grandement basé sur des tableau avec peut de class et id).
]]></description>
		<content:encoded><![CDATA[<p>Pourquoi ne pas utiliser lxml?</p>
<p>Ça permettrais de faire des recherches avec xpath, qui est quand même plus simple/souple surtout pour des sites mal foutu (par exemple grandement basé sur des tableau avec peut de class et id).</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Sam</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-20726</link>
		<dc:creator><![CDATA[Sam]]></dc:creator>
		<pubDate>Tue, 04 Mar 2014 09:41:30 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-20726</guid>
		<description><![CDATA[Corrigé pour findall. Par contre findAll est bien un alias de find_all.]]></description>
		<content:encoded><![CDATA[<p>Corrigé pour findall. Par contre findAll est bien un alias de find_all.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Feadurn</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-20712</link>
		<dc:creator><![CDATA[Feadurn]]></dc:creator>
		<pubDate>Tue, 04 Mar 2014 01:45:03 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-20712</guid>
		<description><![CDATA[Une petite erreur dans l&#039;article, un coup c&#039;est &lt;code&gt;soup.findall(&#039;div&#039;)&lt;/code&gt; la fois suivante c&#039;est &lt;code&gt;soup.findAll(&#039;div&#039;)&lt;/code&gt; alors que cela devrait etre &lt;code&gt;soup.find_all()&lt;/code&gt;

Sinon, un grand merci pour cet article qui vient de me sauver un temps fou.]]></description>
		<content:encoded><![CDATA[<p>Une petite erreur dans l&#8217;article, un coup c&#8217;est <code>soup.findall('div')</code> la fois suivante c&#8217;est <code>soup.findAll('div')</code> alors que cela devrait etre <code>soup.find_all()</code></p>
<p>Sinon, un grand merci pour cet article qui vient de me sauver un temps fou.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Remram</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-19058</link>
		<dc:creator><![CDATA[Remram]]></dc:creator>
		<pubDate>Tue, 28 Jan 2014 15:49:58 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-19058</guid>
		<description><![CDATA[J&#039;ai été récemment surpris, alors que je cherchais un moyen sans prise de tête de &quot;downgrader&quot; du HTML en text/plain, que BeautifulSoup n&#039;avait aucun moyen préconçu de faire ça. J&#039;ai fini par utiliser &lt;a href=&quot;http://git.io/html2text&quot; rel=&quot;nofollow&quot;&gt;html2text&lt;/a&gt; d&#039;Aaron Swartz.

J&#039;ai raté un truc ? (me renvoyez pas vers get_text, &lt;a href=&quot;http://git.io/py_html2text&quot; rel=&quot;nofollow&quot;&gt;ça pue&lt;/a&gt;)]]></description>
		<content:encoded><![CDATA[<p>J&#8217;ai été récemment surpris, alors que je cherchais un moyen sans prise de tête de &#8220;downgrader&#8221; du HTML en text/plain, que BeautifulSoup n&#8217;avait aucun moyen préconçu de faire ça. J&#8217;ai fini par utiliser <a href="http://git.io/html2text" rel="nofollow">html2text</a> d&#8217;Aaron Swartz.</p>
<p>J&#8217;ai raté un truc ? (me renvoyez pas vers get_text, <a href="http://git.io/py_html2text" rel="nofollow">ça pue</a>)</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: k3c</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-18996</link>
		<dc:creator><![CDATA[k3c]]></dc:creator>
		<pubDate>Sun, 26 Jan 2014 20:53:32 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-18996</guid>
		<description><![CDATA[Vérifie si rtmpdump et librtmpdump sont bien installés, et si la version  est la même pour les 2 (la 2.4 pour moi). Tu peux aussi faire un test rapide, wget d&#039;une Ubuntu 13.10 32 bits, puis
qemu-img create -f qcow2 Ubuntu_1304_image.img 6G
puis 
kvm -m 756 -cdrom Téléchargements/ubuntu-13.04-desktop-i386.iso -boot d Ubuntu_1304_image.img
installation, reboot
kvm -m 756  Ubuntu_1304_image.img
installation de rtmpdum et librtmpdump, et test de la commande rtmpdump donnée dans l&#039;article.
rm du .img quand tu as fini.]]></description>
		<content:encoded><![CDATA[<p>Vérifie si rtmpdump et librtmpdump sont bien installés, et si la version  est la même pour les 2 (la 2.4 pour moi). Tu peux aussi faire un test rapide, wget d&#8217;une Ubuntu 13.10 32 bits, puis<br />
qemu-img create -f qcow2 Ubuntu_1304_image.img 6G<br />
puis<br />
kvm -m 756 -cdrom Téléchargements/ubuntu-13.04-desktop-i386.iso -boot d Ubuntu_1304_image.img<br />
installation, reboot<br />
kvm -m 756  Ubuntu_1304_image.img<br />
installation de rtmpdum et librtmpdump, et test de la commande rtmpdump donnée dans l&#8217;article.<br />
rm du .img quand tu as fini.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: tester</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-18959</link>
		<dc:creator><![CDATA[tester]]></dc:creator>
		<pubDate>Sat, 25 Jan 2014 20:08:00 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-18959</guid>
		<description><![CDATA[Merci pour la réponse k3c ;)
En fait c&#039;est carrément la bande annonce que je n&#039;arrive pas à récupérer, ça bloque à 2.60% chez moi avec un retour erreur 
&lt;code&gt;Download may be incomplete (downloaded about 2.60%), try resuming&lt;/code&gt;
L&#039;argument -e n&#039;y fait rien...j&#039;ai toujours galèrer avec rtmpdump donc je voulais savoir si ça venait de chez moi ou pas (visiblement oui...).]]></description>
		<content:encoded><![CDATA[<p>Merci pour la réponse k3c ;)<br />
En fait c&#8217;est carrément la bande annonce que je n&#8217;arrive pas à récupérer, ça bloque à 2.60% chez moi avec un retour erreur<br />
<code>Download may be incomplete (downloaded about 2.60%), try resuming</code><br />
L&#8217;argument -e n&#8217;y fait rien&#8230;j&#8217;ai toujours galèrer avec rtmpdump donc je voulais savoir si ça venait de chez moi ou pas (visiblement oui&#8230;).</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: k3c</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-18957</link>
		<dc:creator><![CDATA[k3c]]></dc:creator>
		<pubDate>Sat, 25 Jan 2014 18:56:56 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-18957</guid>
		<description><![CDATA[La vidéo de D8 est une bande-annonce, donc courte. En matière de replay, on ne sait pas récupérer uniquement les vidéos de M6 qui utilisent du Flash Access (Protected Http Dynamic Streaming). La plupart des sites de replay se téléchargent avec rtmpdump ou AdobeHDS.php, certains encore plus simples, avec juste un wget/curl/msdl. Les sites étrangers demandent un proxy en général uniquement pour trouver l&#039;adresse de la vidéo, ensuite la commande à passer n&#039;a pas besoin de proxy.]]></description>
		<content:encoded><![CDATA[<p>La vidéo de D8 est une bande-annonce, donc courte. En matière de replay, on ne sait pas récupérer uniquement les vidéos de M6 qui utilisent du Flash Access (Protected Http Dynamic Streaming). La plupart des sites de replay se téléchargent avec rtmpdump ou AdobeHDS.php, certains encore plus simples, avec juste un wget/curl/msdl. Les sites étrangers demandent un proxy en général uniquement pour trouver l&#8217;adresse de la vidéo, ensuite la commande à passer n&#8217;a pas besoin de proxy.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: tester</title>
		<link>http://sametmax.com/parser-du-html-avec-beautifulsoup/#comment-18949</link>
		<dc:creator><![CDATA[tester]]></dc:creator>
		<pubDate>Sat, 25 Jan 2014 15:11:03 +0000</pubDate>
		<guid isPermaLink="false">http://sametmax.com/?p=6161#comment-18949</guid>
		<description><![CDATA[Elles ne sont pas fragmentées les videos dans la VOD de D8 ? Parce que la commande rtpmdump me renvoie qu&#039;une portion...]]></description>
		<content:encoded><![CDATA[<p>Elles ne sont pas fragmentées les videos dans la VOD de D8 ? Parce que la commande rtpmdump me renvoie qu&#8217;une portion&#8230;</p>
]]></content:encoded>
	</item>
</channel>
</rss>
