<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" >

<channel>
	<title>celery &#8211; Sam &amp; Max</title>
	<atom:link href="http://sametmax.com/tag/celery/feed/" rel="self" type="application/rss+xml" />
	<link>http://sametmax.com</link>
	<description>Du code, du cul</description>
	<lastBuildDate>Thu, 05 Sep 2019 08:22:03 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.7</generator>
<site xmlns="com-wordpress:feed-additions:1">32490438</site>	<item>
		<title>La stack techno qu&#8217;on utilise pour faire un site Web, et pourquoi</title>
		<link>http://sametmax.com/la-stack-techno-quon-utilise-pour-faire-un-site-web-et-pourquoi/</link>
		<comments>http://sametmax.com/la-stack-techno-quon-utilise-pour-faire-un-site-web-et-pourquoi/#comments</comments>
		<pubDate>Mon, 11 Nov 2013 06:40:38 +0000</pubDate>
		<dc:creator><![CDATA[Sam]]></dc:creator>
				<category><![CDATA[Administration System]]></category>
		<category><![CDATA[celery]]></category>
		<category><![CDATA[django]]></category>
		<category><![CDATA[fabric]]></category>
		<category><![CDATA[mysql]]></category>
		<category><![CDATA[nginx]]></category>
		<category><![CDATA[nosql]]></category>
		<category><![CDATA[postgres]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[redis]]></category>
		<category><![CDATA[vm]]></category>

		<guid isPermaLink="false">http://sametmax.com/?p=7648</guid>
		<description><![CDATA[Une stack techno n'est pas une référence. Il n'y a pas de combo absolu qui rox absolument tout, c'est une question de contexte technique, financier, humain...

Mais c'est vrai que ça aide bien d'avoir sous les yeux les pratiques des autres.]]></description>
				<content:encoded><![CDATA[<p>Une stack techno n&#8217;est pas une référence. Il n&#8217;y a pas de combo absolu qui rox absolument tout, c&#8217;est une question de contexte technique, financier, humain&#8230;</p>
<p>Mais c&#8217;est vrai que ça aide bien d&#8217;avoir sous les yeux les pratiques des autres.</p>
<p>Je ne vais pas expliquer pourquoi Python, <a href="http://sametmax.com/10-raisons-pour-lesquelles-je-suis-toujours-marie-a-python/">je l&#8217;ai déjà fait</a>.</p>
<p>Commençons plutôt par la partie purement Web, pour laquelle on utilise Django, le framework Web Python.</p>
<p>Max et moi avons tout deux fait du PHP avant, j&#8217;ai tâté des frameworks internes, du Symfony et plus tard du Zope. J&#8217;ai regardé du côté de Pyramid et de ses prédécesseurs, et Django est celui qui me plaît le plus. J&#8217;ai juste un peu forcé la main à Max :-)</p>
<p>Car oui, le framework a été avant tout un choix de goût.</p>
<p>Ce n&#8217;est pas un choix de performances : le framework n&#8217;a aucun impact dessus. Aucun. Les architectures ont un impact. Le framework, non. Votre bottleneck sera sur les IO, pas sur le CPU. Le choix de technos asynchrones peut avoir un impact, mais ce n&#8217;est pas une question de framework. Tornado, Twisted ou NodeJS, on s&#8217;en fout. </p>
<p>Donc Django, essentiellement parce qu&#8217;il me plait. Et il me plaît pour ces raisons :</p>
<ul>
<li>Il y a un bon équilibre entre découplage et intégration. En général c&#8217;est soit très découplé et mal intégré, soit très bien intégré et très couplé.</li>
<li>C&#8217;est bien foutu et bien documenté. Et c&#8217;est stable. Vraiment très stable. Les core devs sont hyper sérieux.</li>
<li>C&#8217;est très versatile et ça peut faire plein de trucs out of the box, petits comme gros.</li>
<li>C&#8217;est assez facile à apprendre. Ça reste un framework, donc ce n&#8217;est pas la plus simple des démarches, mais dans le royaume des frameworks de cette taille, ça reste vraiment le plus simple.</li>
<li>La communauté est fantastique : il y a des centaines d&#8217;apps qui couvrent pratiquement tous les besoins.</li>
<li>Et bien entendu, c&#8217;est en Python.</li>
</ul>
<p>En terme de base de données, on a fait du MySQL pendant longtemps. Ça a plutôt bien marché. Maintenant je commence mes nouveaux projets avec PostGres, qui est plus solide. Parfois je fais juste du Sqlite, parce que ça suffit.</p>
<p>Pas de NoSQL. Après plusieurs expériences avec MongoDB et CouchDB, je n&#8217;ai pas été convaincu que les bénéfices dépassaient le coût. Il faudrait un article complet là-dessus (qu&#8217;on m&#8217;a d&#8217;ailleurs demandé).</p>
<p>Question OS. c&#8217;est du CentOS avec Max (il a plus l&#8217;habitude) ou du Ubuntu Server pour mes autres projets. Je reste sur les LTS. Ce n&#8217;est pas un choix très réfléchi, c&#8217;est surtout par habitude.</p>
<p>Pas de machine virtuelle. On a essayé, sans y trouver un grand intérêt :</p>
<ul>
<li>Il faut quand même faire des scripts de migration, donc autant s&#8217;en servir pour le déploiement.</li>
<li>On perd en perfs.</li>
<li>Les erreurs liées au mal-fonctionnement d&#8217;une VM sont absolument indébuggables.</li>
<li>Si on ne fait pas la VM soit-même, il faut mettre ses couilles dans les mains d&#8217;un prestataire de service. J&#8217;ai horreur de ça.</li>
<li>Trouver des gens avec la compétence pour gérer une VM, c&#8217;est difficile. Un script de déploiement, c&#8217;est du code que tout dev saura déjà lire. Par extension ça veut dire que je m&#8217;y replonge facilement des semaines plus tard.</li>
</ul>
<p>Et donc pour le déploiement, j&#8217;utilise <a href="http://sametmax.com/travailler-moins-pour-gagner-plus-en-15-minutes-avec-python-fabric/">fabric</a>, avec <a href="https://github.com/ronnix/fabtools/">fabtools</a>.</p>
<p>Ce n&#8217;est pas la solution la plus efficace, d&#8217;autant que ça limite à Python 2.7, mais c&#8217;est la plus simple. C&#8217;est juste du code Python. N&#8217;importe qui peut comprendre le déploiement en 15 minutes. Ça se modifie vite, s&#8217;adapte facilement. </p>
<p>Il faut comprendre qu&#8217;on a jamais plus d&#8217;une dizaine de serveurs pour un projet, ces choix sont donc faits en fonction de cela. Il va sans dire que si vous gérez un parc de centaines de machines, ça ne sera pas du tout le même choix technique. Peut être que Chef ou des VM seront alors carrément plus intéressants. Peut être que le NoSQL et sa capacité au scaling sera bien plus rentable.</p>
<p>Il ne s&#8217;agit pas de décrier les technos que nous n&#8217;utilisons pas. Il s&#8217;agit juste de dire, voilà les choix que nous avons faits, dans tel contexte, pour telles (bonnes ou mauvaises) raisons.</p>
<p>Durant les dernières années, on a ajouté <a href="http://redis.io/">Redis</a> à notre stack. C&#8217;est un outil fantastique qui sert à tout : de la base de données pour les trucs simples (il y a des fois ou un schéma est overkill) à la solution de caching. C&#8217;est ce qu&#8217;on a de plus proche du NoSQL.</p>
<p>L&#8217;outil est tellement simple à installer (vraiment le degré zero de la maintenance, c&#8217;est beau) et à utiliser que ça ne vaut juste pas le coup de s&#8217;en priver. </p>
<p>Du coup, plus de memcache. Toutes les grosses requêtes sont sauvegardées dans Redis, dès qu&#8217;on fait un script qui a besoin de persistance temporaire, Redis, pour communiquer entre plusieurs process, Redis, pour toutes les opérations qui ont besoin de grosses perfs comme les stats, Redis. Vive Redis.</p>
<p>D&#8217;ailleurs on utilise Redis aussi comme broker pour notre gestionnaire de queues et de taches : <a href="http://sametmax.com/files-de-taches-et-taches-recurrentes-avec-celery/">celery</a>. Si vous pythonez, je vous recommande chaudement celery pour toutes les tâches en background, les crawlers, les chaînes de process, etc.</p>
<p>On a aussi du moteur de recherche. Là on tape dans du <a href="http://lucene.apache.org/solr/">Solr</a> (avec <a href="http://haystacksearch.org/">haystack</a>). C&#8217;est très puissant, en tout cas syntaxiquement car ça ne fait pas de sémantique. Ne vous attendez donc pas à rattraper Google. Mais c&#8217;est aussi méga chiant à configurer et très lourd. Je pense qu&#8217;un jour on va migrer sur <a href="http://www.elasticsearch.org/">ElasticSearch</a>, mais c&#8217;est pas la priorité. Don&#8217;t fix what ain&#8217;t broken.</p>
<p>Devant tout ça on a <a href="http://nginx.org/">Nginx</a>. Comme beaucoup on a fait Apache => Cherokee => lighttp => nginx. Et franchement, je ne reviendrai jamais en arrière : plus léger, plus rapide, plus facile à installer et à configurer, plus versatile. Nginx fait tout, et mieux. </p>
<p>En proxy on a du <a href="http://gunicorn.org/">gunicorn</a>. Parce qu&#8217;on avait la flemme de configurer uwsgi et qu&#8217;on a pris l&#8217;habitude.</p>
<p>Après on utilise plein de libs, de petits outils, etc. Mais ça c&#8217;est le gros de notre archi.</p>
]]></content:encoded>
			<wfw:commentRss>http://sametmax.com/la-stack-techno-quon-utilise-pour-faire-un-site-web-et-pourquoi/feed/</wfw:commentRss>
		<slash:comments>33</slash:comments>
	<post-id xmlns="com-wordpress:feed-additions:1">7648</post-id><enclosure url="http://sametmax.com/wp-content/uploads/2013/11/nZ9b9.jpg" length="67160" type="image/jpg" />	</item>
		<item>
		<title>Files de tâches et tâches récurrentes avec Celery</title>
		<link>http://sametmax.com/files-de-taches-et-taches-recurrentes-avec-celery/</link>
		<comments>http://sametmax.com/files-de-taches-et-taches-recurrentes-avec-celery/#comments</comments>
		<pubDate>Sat, 27 Jul 2013 07:57:12 +0000</pubDate>
		<dc:creator><![CDATA[Sam]]></dc:creator>
				<category><![CDATA[Programmation]]></category>
		<category><![CDATA[celery]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[redis]]></category>

		<guid isPermaLink="false">http://sametmax.com/?p=6880</guid>
		<description><![CDATA[Quand on a traiter des choses bloquantes, avec des dépendances, des flux complexes ou des actions répétitives, créer des files d'attente peut se révéler très judicieux.
]]></description>
				<content:encoded><![CDATA[<p>Quand on a à traiter des choses bloquantes, avec des dépendances, des flux complexes ou des actions répétitives, créer des files d&#8217;attente peut se révéler très judicieux.</p>
<p>Par exemple lancer la génération d&#8217;un gros zip sur le clic d&#8217;un utilisateur, télécharger plein fichiers en parallèle pour son site de cul, lancer des calculs sur plusieurs machines et récupérer le résultat, encoder des videos en arrière plan, etc.</p>
<p>Le problème, c&#8217;est que fabriquer des files d&#8217;attente à la main, ça mène généralement à une grosse galère. La première boîte dans laquelle j&#8217;ai travaillé avait tout un système de queues à base de PHP + SQL fait à la main qui tapait dans du MySQL, c&#8217;était pas marrant du tout</p>
<p>Je fais une pause, et je note que le potentiel de jeux de mots sur cet article est fortement élevé. Mais je resterai fort.</p>
<p>Locking, priorité, dépendance, asynchronicité, concurrence, sérialisation, encoding, stockage, accessibilité, load balancing&#8230; Toutes ces problématiques sont bien vicieuses et chronophages. Il vaut mieux utiliser une lib solide et éprouvée.</p>
<p>Je resterai fort.</p>
<p><a href="https://pypi.python.org/pypi/kombu/2.2.0">Kombu</a> est une telle lib, mais elle est lourde et complexe à utiliser. J&#8217;avais fais le choix de la prendre pour un gros projet avec Max, je le regrette sur le long terme : c&#8217;est dur à maintenir et à faire évoluer. Le code est vraiment pas sympa.</p>
<p>Heureusement il existe une bibliothèque qui se met au dessus de Kombu pour et nous expose juste les fonctionnalités que l&#8217;on souhaite : <a href="http://www.celeryproject.org/">celery</a>.</p>
<p>Fort.</p>
<p>Celery est simple pour démarrer, mais très puissant si on rentre dans le détail, et croyez moi, le détail, on peut y rentrer très très profondément.</p>
<p>F&#8230;</p>
<h2>Installation</h2>
<p>Qui dit file d&#8217;attente, dit stockage. Il faut bien mettre les tâches quelque part et communiquer avec ce quelque part. En base de données ? Dans un gestionnaire de messages ? En mémoire ? Dans un cache ?</p>
<p>Celery résout le problème en proposant la même interface, quelque soit le support. Actuellement, on peut utiliser :</p>
<ul>
<li>RabbitMQ</li>
<li>Redis</li>
<li>MongoDB</li>
<li>Beanstalk CouchDB</li>
<li>SQLAlchemy ou l&#8217;ORM Django (et donc toutes les bases de données supportées comme Sqlite, MySQL, PostGres&#8230;)</li>
<li>Amazon SQS</li>
</ul>
<p>Dans notre exemple, nous allons le faire avec Redis car :</p>
<ul>
<li>Redis est très simple à installer et à configurer.</li>
<li>Nous on utilise déjà du Redis partout.</li>
<li>Aucun risque de locking.</li>
</ul>
<p>Pour ceux qui ont pas redis, c&#8217;est généralement dans les dépôts. Par exemple sur Ubuntu :</p>
<pre lang="bash">sudo apt-get install redis-server</pre>
<p>Il n&#8217;y a rien à faire de plus, ça tourne, c&#8217;est configuré avec des valeurs par défaut qui sont saines. Je vous l&#8217;ai dis, redis, c&#8217;est fantastiquement bien foutu.</p>
<p>Ensuite on install celery et la lib d&#8217;accès à redis en Python qui porte un nom très original :</p>
<pre lang="bash">pip install celery redis</pre>
<p>Ca devrait compiler un peu, et comme hab avec les extensions en C, assurez vous d&#8217;avoir un compilateur et les headers en place comme indiqué dans l&#8217;article sur <a href="http://sametmax.com/votre-python-aime-les-pip/">pip</a>.</p>
<p>Ensuite on peut créer ses tâches. Créez un module, par exemple <em>tasks.py</em> :</p>
<pre lang="python">import urllib2

from collections import Counter

from celery import Celery

# Configuration de celery. Ceci peut aussi se faire dans un fichier de config.
# Ici on dit à celery que pour le module 'tasks', on va utiliser redis
# comme broker (passeur de massage) et comme result backend (stockage du
# resultat des tâches).
celery = Celery('tasks', broker='redis://localhost', backend='redis://localhost')


# Et voici notre première tâche. C'est une fonction Python normale, décorée
# avec un decorateur de celery. Elle prend une URL, et calcule le nombre
# de lettre "e" qu'il y a dans la page.
@celery.task
def ecount(url):
    return Counter(urllib2.urlopen(url).read())['e']</pre>
<p>On lance ensuite le processus celery dans un terminal (en production, mettez ça dans supervisord ou systemd pour que ça démarre automatiquement) :</p>
<pre>
[test] sam ~/Bureau/celery_test $ celery -A tasks worker -B --loglevel=info

 -------------- celery@sam v3.0.21 (Chiastic Slide)
---- **** -----
--- * ***  * -- Linux-3.2.0-48-generic-x86_64-with-Ubuntu-12.04-precise
-- * - **** ---
- ** ---------- [config]
- ** ---------- .> broker:      redis://localhost:6379//
- ** ---------- .> app:         tasks:0x2a2fa50
- ** ---------- .> concurrency: 4 (processes)
- *** --- * --- .> events:      OFF (enable -E to monitor this worker)
-- ******* ----
--- ***** ----- [queues]
 -------------- .> celery:      exchange:celery(direct) binding:celery


[Tasks]
  . tasks.ecount

[2013-07-26 13:22:21,631: INFO/Beat] Celerybeat: Starting..
[2013-07-26 13:04:51,274: WARNING/MainProcess] celery@sam ready.
[2013-07-26 13:04:51,280: INFO/MainProcess] consumer: Connected to redis://localhost:6379//.
</pre>
<p><code>-A</code> précise le module à importer, <code>-B</code> démarre le beat (on verra ça plus tard), <code>worker</code> dit à celery que démarrer des processus de consommation de files d&#8217;attente (par défaut 4 qui travaillent en parallèle), et <code>--loglevel=info</code> va nous permettre d&#8217;avoir un affichage verbeux pour comprendre ce qui se passe.</p>
<p>Votre file d&#8217;attente est prête, et frétille d&#8217;impatience.</p>
<h2>Lancer une tâche</h2>
<p>A partir de là, vous pouvez envoyer des tâches dans la file d&#8217;attente, depuis n&#8217;importe où :</p>
<ul>
<li>Un script.</li>
<li>Un serveur Web (par exemple une vue Django).</li>
<li>Un programme sur un autre serveur (même si il faudrait alors configurer redis pour qu&#8217;il écoute sur les ports extérieurs, ce qui n&#8217;est pas le cas ici par simplicité).</li>
<li>etc</li>
</ul>
<p>Plusieurs programmes peuvent envoyer plein de tâches, en même temps, et elles vont se loger dans la file d&#8217;attente, sans bloquer le programme qui les a envoyé.</p>
<p>Par exemple, depuis le shell :</p>
<pre lang="python">>>> from tasks import ecount
>>> res = ecount.delay('http://danstonchat.com')</pre>
<p>Ceci ne bloque pas mon shell, la ligne s&#8217;exécute immédiatement. La fonction <code>ecount</code> n&#8217;est pas appelée depuis le shell, elle est dans la file d&#8217;attente et sera appelée par un des processus (les fameux &#8216;worker&#8217;) qui consomment la queue. Du côté de la file, on peut voir dans le log :</p>
<pre lang="bash">[2013-07-26 14:18:08,609: INFO/MainProcess] Got task from broker: tasks.ecount[599a52ea-ef6b-4499-981d-cd17fab592df]
[2013-07-26 14:18:09,070: INFO/MainProcess] Task tasks.ecount[599a52ea-ef6b-4499-981d-cd17fab592df] succeeded in 0.446974039078s: 1242</pre>
<p>On a donc notre tâche qui a bien été traitée.</p>
<p>On peut récupérer le résultat dans le shell :</p>
<pre lang="python">>>> res.state
'PENDING'
</pre>
<p>Ah&#8230; La tâche n&#8217;est pas encore terminée. Et un peu plus tard :</p>
<pre lang="python">>>> res.state
'SUCCESS'
>>> res.result
1242</pre>
<p>Lancer une tâche est bien entendu peu intéressant, les listes d&#8217;attente sont vraiment sympa quand on a plein de tâches à lancer, par plein de processus différents :</p>
<pre lang="python">results = [ecount.delay(url) for url in ('http://google.com', 'http://sametmax.com', 'http://sebsauvage.com', 'http://multiboards.com', 'http://0bin.net', 'http://danstonchat.com')]
</pre>
<pre lang="bash">[2013-07-26 14:25:46,646: INFO/MainProcess] Got task from broker: tasks.ecount[5d072a7b-29f8-4ea6-8d92-6a4c1740d724]
[2013-07-26 14:25:46,649: INFO/MainProcess] Got task from broker: tasks.ecount[402f6a4f-6b35-4f62-a786-9a5ba27707d2]
[2013-07-26 14:25:46,650: INFO/MainProcess] Got task from broker: tasks.ecount[bbe46b1b-4719-4c42-bd2f-21e4d72e613e]
[2013-07-26 14:25:46,652: INFO/MainProcess] Got task from broker: tasks.ecount[8fb35186-66e2-4eae-a40c-fc42e500ab9d]
[2013-07-26 14:25:46,653: INFO/MainProcess] Got task from broker: tasks.ecount[fc63f5db-8ade-4383-b719-c3d6390ca246]
[2013-07-26 14:25:46,654: INFO/MainProcess] Got task from broker: tasks.ecount[8434e21d-79ea-4559-a90e-92e2bc2b9dc7]
[2013-07-26 14:25:47,144: INFO/MainProcess] Task tasks.ecount[bbe46b1b-4719-4c42-bd2f-21e4d72e613e] succeeded in 0.479865789413s: 27
[2013-07-26 14:25:47,242: INFO/MainProcess] Task tasks.ecount[5d072a7b-29f8-4ea6-8d92-6a4c1740d724] succeeded in 0.578661203384s: 609
[2013-07-26 14:25:47,501: INFO/MainProcess] Task tasks.ecount[fc63f5db-8ade-4383-b719-c3d6390ca246] succeeded in 0.35736989975s: 263
[2013-07-26 14:25:47,645: INFO/MainProcess] Task tasks.ecount[8434e21d-79ea-4559-a90e-92e2bc2b9dc7] succeeded in 0.403187036514s: 1270
[2013-07-26 14:25:47,815: INFO/MainProcess] Task tasks.ecount[8fb35186-66e2-4eae-a40c-fc42e500ab9d] succeeded in 1.14100408554s: 23
[2013-07-26 14:25:49,010: INFO/MainProcess] Task tasks.ecount[402f6a4f-6b35-4f62-a786-9a5ba27707d2] succeeded in 2.34633708s: 3158
</pre>
<p>Car du coup on sait que ces multiples tâches ne vont pas bloquer le processus en cour, mais qu&#8217;en plus la charge sera répartie sur le nombre de workers qu&#8217;on a décidé au départ, ni plus (surcharge du serveur), ni moins (traitement trop lent).</p>
<h2>Comment je sais quand une tâche est terminée ?</h2>
<p>On peut attendre que la tâche soit terminée :</p>
<pre lang="python">>>> print res.wait()
9999</pre>
<p>Mais ce n&#8217;est pas vraiment le but. On cherche avant tout à ce que les tâches soient non bloquantes, et exécutées dans un processus à part voir potentiellement distribuées sur plusieurs serveurs. </p>
<p>Par ailleurs, Celery n&#8217;est pas un remplacement d&#8217;un système de traitement asynchrone comme Tornado ou NodeJS, il n&#8217;est pas fait pour envoyer des réponses asynchrones à l&#8217;utilisateur. Il est fait pour faire des tâches en background, répartir la charge et ordonner le traitement. Bien entendu, on peut faire communiquer un système asynchrone avec celery comme <a href="https://github.com/mher/tornado-celery">ici</a> ou <a href="http://cyberfart.blogspot.fr/2012/11/tornado-celery-integration.html">ici</a>, mais c&#8217;est une autre histoire.</p>
<p>Concentrons nous sur les tâches.</p>
<p>La question de &#8220;Comment je sais quand une tâche est terminée ?&#8221; est souvent traduisible par &#8220;comment je réagis à une tâche pour lancer du code quand elle s&#8217;est terminée sans erreur ?&#8221;.</p>
<p>Et là, il y une solution toute simple :</p>
<pre lang="python">res = tache1.s(arg1, arg2) | tache2.s() | tache3.s(arg1)</pre>
<p>Ceci va créer une chaîne de tâches. Quand la première se termine, la deuxième se lance en recevant le résultat de la première en argument.</p>
<p><code>s()</code> fabrique une sous-tâche, c&#8217;est à dire une tâche à envoyer dans la file plus tard avec des arguments pré-enregistrés. Dans notre exemple, celery va lancer <code>tache1</code> avec deux arguments, puis si ça marche, va appeler <code>tache2</code> en lui passant le résultat de <code>tache1</code> comme argument, puis si ça marche, va appeler <code>tache3</code> avec le résultat de <code>tache2</code> en premier argument et <code>arg1</code> en second argument.</p>
<p>En fait, celery vient avec tout <a href="http://docs.celeryproject.org/en/latest/userguide/canvas.html?highlight=group">un tas d&#8217;outils</a> pour exécuter des tâches dépendantes les unes des autres : par groupes, par chaînes, par morceaux, etc. Mais de toute façon, vous pouvez appeler une tâche&#8230; à l&#8217;intérieur d&#8217;une autre tâche. Donc parti de là vous pouvez faire pas mal de choses.</p>
<h2>Comment je fais pour faire une tâche récurrente ?</h2>
<p>C&#8217;est là qu&#8217;intervient le &#8220;beat&#8221; dont j&#8217;ai parlé tout à l&#8217;heure. Avec cette option, celery va vérifier toutes les secondes si il n&#8217;y a pas une tâche répétitive à lancer, et la mettre dans une file d&#8217;attente, à la manière d&#8217;un cron.</p>
<p>Il suffit de définir une tâche comme <code>periodic_task</code> pour qu&#8217;elle soit lancée régulièrement.</p>
<pre lang="python">import smtplib

from celery.schedules import crontab
from celery.decorators import periodic_task

# va executer la tâche à 5h30, 13h30 et 23h30 tous les lundi
# run_every accepte aussi un timedelta, pour par exemple dire "toutes les 10m"
@periodic_task(run_every=crontab(hour='5,13,23', minute=30, day_of_week='monday'))
def is_alive():
    """
        Vérifie que le blog est toujours en ligne, et si ce n'est pas le cas,
        envoie un mail en panique.
    """
    if urllib2.urlopen('http://sametmax.com').code != 200:
        mail = 'lesametlemax__AT__gmail.com'.replace('__AT__', '@')
        server = smtplib.SMTP('smtp.gmail.com:587')
        server.starttls()
        server.login('root', 'admin123')
        server.sendmail(mail, mail, msg)
        server.quit()
</pre>
<p>Il y a de <a href="http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html?highlight=cron">bons exemples</a> sur la syntaxe sur <code>crontab()</code> dans la doc.</p>
<p>D&#8217;une manière générale, <a href="http://docs.celeryproject.org/en/latest/getting-started/introduction.html">la doc de Celery est très très riche</a>, donc plongez vous dedans si cet article ne répond pas à vos besoins, car si ça peut être mis dans une file, ça peut être fait par Celery.</p>
<h2>Note de fin</h2>
<p>Celery n&#8217;autoreload pas le code, donc redémarrez les workers à chaque fois que vous modifiez vos tasks.</p>
<p>Attention aussi aux tâches récurrentes, la suivante peut se lancer avant que la précédente soit terminée. C&#8217;est à vous de faire des tâches <a href="https://fr.wikipedia.org/wiki/Idempotent">idempotentes</a>, ou alors de mettre en place <a href="http://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html#cookbook-task-serial">un système de locking</a>.</p>
]]></content:encoded>
			<wfw:commentRss>http://sametmax.com/files-de-taches-et-taches-recurrentes-avec-celery/feed/</wfw:commentRss>
		<slash:comments>29</slash:comments>
	<post-id xmlns="com-wordpress:feed-additions:1">6880</post-id><enclosure url="http://sametmax.com/wp-content/uploads/2013/07/zdFRjr1.jpg" length="76872" type="image/jpg" />	</item>
		<item>
		<title>Travail distribué en Python SANS Celery</title>
		<link>http://sametmax.com/travail-distribue-en-python-sans-celey/</link>
		<comments>http://sametmax.com/travail-distribue-en-python-sans-celey/#comments</comments>
		<pubDate>Tue, 29 May 2012 03:04:37 +0000</pubDate>
		<dc:creator><![CDATA[Sam]]></dc:creator>
				<category><![CDATA[Administration System]]></category>
		<category><![CDATA[Programmation]]></category>
		<category><![CDATA[celery]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[queues]]></category>

		<guid isPermaLink="false">http://sametmax.com/?p=765</guid>
		<description><![CDATA[L'équipe de Disqus a sorti un outil pratique pour faire du traitement parallèle sans se prendre la tête.]]></description>
				<content:encoded><![CDATA[<p>[three_fourth_last]Quand on veut effectuer une même tâche un très grand nombre de fois, on peut utiliser une boucle comme un gros bourrin et effectuer la tâches dedans.</p>
<p>Mais il faut alors gérer:</p>
<ul>
<li>le cas où le processus s&#8217;interrompt et sa reprise;</li>
<li>le parallélisme.</li>
</ul>
<p>Le premier cas n&#8217;est guère que du travail en plus, et pas insurmontable. Le deuxième cas est plus compliqué: les threads sont inéficaces en Python pour tout ce qui n&#8217;est pas limité par l&#8217;IO et l&#8217;alternative d&#8217;utiliser des processus séparés est énormément de boulot.</p>
<p>Du coup généralement on ne le fait pas, et tout est traité en une fois, alors qu&#8217;on pourrait aller 4 fois plus vite avec 4 traitements en parallèle.</p>
<h2>Kombu et Celery</h2>
<p>Il existe des outils très puissants pour faire des queues en Python, notamment la library <a href="http://celeryproject.org/">Celery</a>, qui utilise elle-même la bibliothèque <a href="http://pypi.python.org/pypi/kombu">Kombu</a>. Mais quand on veut juste faire un petit job rapide, les mettre en oeuvre, c&#8217;est pas mal de temps. Celery brille pour mettre dans une queue plein de tâches issues de processus externes concurrents, mais rapidement mettre en place un traitement de masse, ce n&#8217;est pas son point fort.</p>
<p>C&#8217;est pour ici qu&#8217;intervient <a href="http://justcramer.com/2012/05/04/distributing-work-without-celery/">un article de David Cramer</a>, de l&#8217;équipe des petits gars de chez <a href="http://disqus.com/">Disqus</a>. Les mecs sont doués, utilisent massivement Django et Python, et chez eux les bases de données ont des milliards d&#8217;entrées. Bref, ils ont plein de choses interessantes à dire.</p>
<p>En l&#8217;occurence, ils ont créé ce petit outil pour leurs migrations: le <a href="https://github.com/dcramer/taskmaster">task master</a>.</p>
<div id="attachment_770" style="width: 446px" class="wp-caption aligncenter"><a href="http://sametmax.com/wp-content/uploads/2012/05/2012-05-28-233600_1024x768_scrot.png" class="grouped_elements" rel="tc-fancybox-group765"><img class=" wp-image-770 " title="Et en prime, on a un bel écran de progression" src="http://sametmax.com/wp-content/uploads/2012/05/2012-05-28-233600_1024x768_scrot.png" alt="Capture d'écran de la sortie de taskmaster" width="436" height="107" /></a><p class="wp-caption-text">Et en prime, on a un bel écran de progression</p></div>
<h2>Utilisation du task master</h2>
<p>Imaginez : vous avez besoin de transcoder des images, renommer des fichiers ou mettre à jour toutes les entrées de votre base de données. Ca va prendre 3 heures, et ça risque de planter. Ce cas de figure est exactement celui dans lequel brille le task master.</p>
<p>On install ça (exemple sous Ubuntu) et avec <a href="http://sametmax.com/votre-python-aime-les-pip/">pip</a>:</p>
<pre lang="bash">
sudo apt-get install libzmq-dev libevent-dev python-dev
pip install taskmaster
</pre>
<p>Puis on créé un fichier <em>tasks.py</em>:</p>
<pre lang="python">def get_jobs(last=0):
    for valeurs_traiter in liste_de_valeurs:
        yield valeurs_traiter

def handle_job(valeurs_traiter):
    print valeurs_traiter</pre>
<p>Les fonctions seront autodétectées si elles portent ce nom.</p>
<p><code>handle_job</code> est la fonction qui va faire votre tâche. Par exemple mettre à jour un objet de votre base de données, encoder une vidéo, etc.</p>
<p>Attention, en cas de reprise des tâches, la première peut être exécutée deux fois. Il faut donc que votre fonction gère ce cas.</p>
<p><code>get_jobs</code> doit retourner un itérable dont chaque élément sera passé à <code>handle_job</code>. Par exemple, <code>yield</code> retournera le nom d&#8217;une vidéo, ou d&#8217;un fichier.</p>
<p>Ensuite on lance le controlleur:</p>
<pre lang="bash">
tm-master tasks.py
</pre>
<p>On on créé autant de workers que l&#8217;on souhaite, ici 4:</p>
<pre lang="bash">
tm-spawn tasks.py 4
</pre>
<p>Dans ce cas, 4 processus vont vider l&#8217;itérable de <code>get_jobs</code> de la liste de valeurs à traiter, et appliquer <code>handle_jobs</code> dessus. Si vous arrêtez le controller ou les workers, vous pouvez les relancer plus tard : ils reprendront là où ils se sont arrêtés.</p>
<p>Si vous avez un processeur avec quatre coeurs, le travail est effectué presque 4 fois plus vite qu&#8217;avec un script tout simple et une boucle <code>for</code>. On peut aussi installer les workers sur des machines séparées, et les faire communiquer avec le controlleur en leur donnant son adresse IP et un port, distribuant ainsi le travail sur de nombreux ordinateurs.</p>
<p>Et en prime, contrairement à celery, toutes les tâches ne sont pas listées en mémoire, mais seulement une partie (que l&#8217;on peut définir en paramètre).</p>
]]></content:encoded>
			<wfw:commentRss>http://sametmax.com/travail-distribue-en-python-sans-celey/feed/</wfw:commentRss>
		<slash:comments>6</slash:comments>
	<post-id xmlns="com-wordpress:feed-additions:1">765</post-id><enclosure url="http://sametmax.com/wp-content/uploads/2012/05/mariage-baccalaureat-mariage-baccalaureat-esclave-sexuelle-parti-dominatrice-fouet-GAME-OVER.png" length="7088" type="image/jpg" />	</item>
	</channel>
</rss>
